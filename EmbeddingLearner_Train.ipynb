{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train word2vec model for embeddings\n",
    "This model will be later used for transfer learning\n",
    "\n",
    "- Window size : 10\n",
    "- Embedding size : 100\n",
    "\n",
    "## Dataset\n",
    "For this model [Stanford Sentiment dataset](https://github.com/HaebinShin/stanford-sentiment-dataset) is used. There are two variants of this dataset available, one for binary classification(positive and negative classes) and multiclass classification (very positive, positive, neutral, negative, very negative). For this task binary classification dataset is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364188, 1224840)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from fastai.text import *\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "path = Path(\"/home/saurabh/Documents/fastai/gradcam_text/data/stanford\")\n",
    "\n",
    "def loadData(path, maxwordCount = 59, empty_word_append=\"<end>\"):\n",
    "    df = pd.DataFrame(columns=['sentement', 'text', 'tokens'])\n",
    "    with open(path) as file:\n",
    "        line = file.readline()\n",
    "        index = 0\n",
    "        while line:\n",
    "            sentement = int(line[0])\n",
    "            text = line[2:]\n",
    "            tokens = text.split()\n",
    "            if len(tokens) > maxwordCount:\n",
    "                print(\"Snipping additional tokens\")\n",
    "                tokens = tokens[0:maxwordCount]\n",
    "            elif len(tokens) < maxwordCount:\n",
    "                tokens.extend([empty_word_append] * (maxwordCount - len(tokens)))\n",
    "                \n",
    "    \n",
    "            df.loc[index] = [sentement, text, tokens]\n",
    "            \n",
    "            \"\"\"Append \"\"\"\n",
    "            line = file.readline()\n",
    "            index+=1\n",
    "    return df\n",
    "\n",
    "\n",
    "df = loadData(path/\"stsa.binary.train\")\n",
    "temp_path = get_tmpfile(path/\"w2v.model\")\n",
    "\n",
    "model = gensim.models.Word2Vec(df.tokens, size=150, window=10, min_count=1, workers=10,  null_word=\"<end>\")\n",
    "model.train(df.tokens, total_examples=df.size, epochs=3000)\n",
    "\n",
    "\"\"\"Save the model for future usage\"\"\"\n",
    "model.save(temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
